import torch
import torch.nn as nn
import torch.nn.functional as F
from tsml_eval._wip.rt.transformations.collection.imbalance.LGD_VAE.src.nn.model import LatentGatedDualVAE
import lightning.pytorch as pl
import torchmetrics
from typing import Tuple
from torchmetrics import Accuracy, F1Score, Recall

# class TSQualityClassifier(pl.LightningModule):
#     def __init__(self, input_channels, num_classes=2, lr=1e-3):
#         super().__init__()
#         self.save_hyperparameters()
#         self.lr = lr
#
#         # å®šä¹‰ç‰¹å¾æå–å™¨
#         self.features = nn.Sequential(
#             nn.Conv1d(input_channels, 64, kernel_size=5, padding=2),
#             nn.BatchNorm1d(64),
#             nn.ReLU(),
#             nn.MaxPool1d(2),
#             nn.Conv1d(64, 128, kernel_size=3, padding=1),
#             nn.BatchNorm1d(128),
#             nn.ReLU(),
#             nn.AdaptiveAvgPool1d(1)
#         )
#         self.classifier = nn.Linear(128, num_classes)
#
#         # åˆå§‹åŒ–è¯„ä¼°æŒ‡æ ‡
#         # num_classes=2 æ—¶ï¼Œtask å¯ä»¥è®¾ä¸º 'multiclass' ä¹Ÿå¯ä»¥è®¾ä¸º 'binary'
#         # è¿™é‡Œç”¨ multiclass ç¡®ä¿é€šç”¨æ€§
#         task = "multiclass"
#         self.acc_metric = Accuracy(task=task, num_classes=num_classes)
#         self.f1_metric = Accuracy(task=task, num_classes=num_classes, average='macro')  # ä¹Ÿå°±æ˜¯ Macro-F1 çš„é€»è¾‘
#         self.f1_macro = F1Score(task=task, num_classes=num_classes, average='macro')
#
#         # G-Means æ˜¯æ¯ä¸ªç±»åˆ«å¬å›ç‡ï¼ˆRecallï¼‰çš„å‡ ä½•å¹³å‡å€¼
#         # æˆ‘ä»¬å…ˆè®°å½•æ¯ä¸ªç±»çš„ Recall
#         self.recall_per_class = Recall(task=task, num_classes=num_classes, average='none')
#
#     def forward(self, x):
#         if x.ndim == 2:  # å¦‚æœæ˜¯ (Batch, Length) è‡ªåŠ¨è¡¥é½é€šé“ç»´åº¦
#             x = x.unsqueeze(1)
#         elif x.shape[1] != self.hparams.input_channels:
#             x = x.transpose(1, 2)
#
#         x = self.features(x)
#         x = torch.flatten(x, 1)
#         return self.classifier(x)
#
#     def training_step(self, batch, batch_idx):
#         x, y = batch
#         logits = self(x)
#         loss = F.cross_entropy(logits, y)
#         return loss
#
#     def validation_step(self, batch, batch_idx):
#         x, y = batch
#         logits = self(x)
#         loss = F.cross_entropy(logits, y)
#         preds = torch.argmax(logits, dim=1)
#
#         # è®¡ç®—æŒ‡æ ‡
#         acc = self.acc_metric(preds, y)
#         f1 = self.f1_macro(preds, y)
#
#         # è®¡ç®— G-Means: å…ˆæ‹¿æ‰€æœ‰ç±»çš„ Recallï¼Œç„¶åæ±‚ä¹˜ç§¯å†å¼€æ–¹
#         recalls = self.recall_per_class(preds, y)
#         g_means = torch.prod(recalls).pow(1 / len(recalls))
#
#         # æ—¥å¿—è®°å½•ï¼Œè¿™æ ·ä½ åœ¨è®­ç»ƒç”Ÿæˆæ¨¡å‹æ—¶å¯ä»¥æ‹¿åˆ°è¿™äº›ç»“æœ
#         metrics = {
#             "val_loss": loss,
#             "val_acc": acc,
#             "val_f1_macro": f1,
#             "val_g_means": g_means
#         }
#         self.log_dict(metrics, prog_bar=True)
#         return metrics
#
#     def configure_optimizers(self):
#         return torch.optim.Adam(self.parameters(), lr=self.lr)


# define the LightningModule


# --- æ ¸å¿ƒè¾…åŠ©å‡½æ•° ---
def FFT_for_Period(x, k=2):
    # [B, T, C]
    xf = torch.fft.rfft(x, dim=1)
    frequency_list = abs(xf).mean(0).mean(-1)
    frequency_list[0] = 0
    _, top_list = torch.topk(frequency_list, k)
    top_list = top_list.detach().cpu().numpy()
    period = x.shape[1] // top_list
    return period, abs(xf).mean(-1)[:, top_list]


# --- ç®€åŒ–ç‰ˆ Inception å— (å¦‚æœä½ æœ‰ç°æˆçš„å¯ä»¥ import) ---
class Inception_Block_V1(nn.Module):
    def __init__(self, in_channels, out_channels, num_kernels=6, init_weight=True):
        super().__init__()
        self.kernels = nn.ModuleList()
        for i in range(num_kernels):
            self.kernels.append(nn.Conv2d(in_channels, out_channels, kernel_size=2 * i + 1, padding=i))
        if init_weight:
            self._initialize_weights()

    def _initialize_weights(self):
        for m in self.modules():
            if isinstance(m, nn.Conv2d):
                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')

    def forward(self, x):
        res_list = [kernel(x) for kernel in self.kernels]
        return torch.mean(torch.stack(res_list, dim=-1), dim=-1)


# --- TimesBlock æ ¸å¿ƒ ---
class TimesBlock(nn.Module):
    def __init__(self, d_model, d_ff, seq_len, top_k, num_kernels):
        super().__init__()
        self.seq_len = seq_len
        self.k = top_k
        self.conv = nn.Sequential(
            Inception_Block_V1(d_model, d_ff, num_kernels=num_kernels),
            nn.GELU(),
            Inception_Block_V1(d_ff, d_model, num_kernels=num_kernels),
        )

    def forward(self, x):
        B, T, N = x.size()
        period_list, period_weight = FFT_for_Period(x, self.k)
        res = []
        for i in range(self.k):
            period = period_list[i]
            if self.seq_len % period != 0:
                length = ((self.seq_len // period) + 1) * period
                padding = torch.zeros([B, (length - self.seq_len), N], device=x.device)
                out = torch.cat([x, padding], dim=1)
            else:
                length = self.seq_len
                out = x
            out = out.reshape(B, length // period, period, N).permute(0, 3, 1, 2).contiguous()
            out = self.conv(out)
            out = out.permute(0, 2, 3, 1).reshape(B, -1, N)
            res.append(out[:, :self.seq_len, :])

        res = torch.stack(res, dim=-1)
        period_weight = F.softmax(period_weight, dim=1).unsqueeze(1).unsqueeze(1).repeat(1, T, N, 1)
        return torch.sum(res * period_weight, -1) + x


# --- æœ€ç»ˆåˆ†ç±»å™¨æ¨¡å— ---
class TimesNetQualityClassifier(pl.LightningModule):
    def __init__(self, input_channels, seq_len, num_classes=2, d_model=64, d_ff=64, top_k=5, e_layers=2, lr=1e-3):
        super().__init__()
        self.save_hyperparameters()
        self.lr = lr

        # 1. çº¿æ€§æŠ•å½±è¿›å…¥ç‰¹å¾ç©ºé—´ (ä»£æ›¿ DataEmbedding ä»¥ç®€åŒ–)
        self.enc_embedding = nn.Linear(input_channels, d_model)

        # 2. å †å  TimesBlock
        self.model = nn.ModuleList([
            TimesBlock(d_model, d_ff, seq_len, top_k, num_kernels=6)
            for _ in range(e_layers)
        ])

        self.layer_norm = nn.LayerNorm(d_model)
        self.act = F.gelu
        self.dropout = nn.Dropout(0.1)

        # 3. è¾“å‡ºå±‚
        self.projection = nn.Linear(d_model * seq_len, num_classes)

        # 4. æŒ‡æ ‡
        task = "multiclass"
        self.acc_metric = Accuracy(task=task, num_classes=num_classes)
        self.f1_macro = F1Score(task=task, num_classes=num_classes, average='macro')
        self.recall_per_class = Recall(task=task, num_classes=num_classes, average='none')

    def forward(self, x):
        # ç¡®ä¿è¾“å…¥æ˜¯ [B, T, C]
        if x.shape[1] == self.hparams.input_channels and x.shape[2] != self.hparams.input_channels:
            x = x.transpose(1, 2)

        # Embedding
        enc_out = self.enc_embedding(x)  # [B, T, d_model]

        # TimesNet å˜æ¢
        for i in range(self.hparams.e_layers):
            enc_out = self.layer_norm(self.model[i](enc_out))

        # Classification Head
        output = self.act(enc_out)
        output = self.dropout(output)
        output = output.reshape(output.shape[0], -1)  # Flatten
        return self.projection(output)

    def training_step(self, batch, batch_idx):
        x, y = batch
        logits = self(x)
        loss = F.cross_entropy(logits, y)
        return loss

    def validation_step(self, batch, batch_idx):
        x, y = batch
        logits = self(x)
        loss = F.cross_entropy(logits, y)
        preds = torch.argmax(logits, dim=1)

        acc = self.acc_metric(preds, y)
        f1 = self.f1_macro(preds, y)
        rec = self.recall_per_class(preds, y)
        g_means = torch.prod(rec).pow(1 / len(rec))

        metrics = {"val_loss": loss, "val_acc": acc, "val_f1_macro": f1, "val_g_means": g_means}
        self.log_dict(metrics, prog_bar=True, sync_dist=True)
        return metrics

    def configure_optimizers(self):
        return torch.optim.Adam(self.parameters(), lr=self.lr)
class LitAutoEncoder(pl.LightningModule):
    def __init__(self,
        in_chans=51,
        seq_len = 100,
        embed_dim=64,
        depth=2,
        num_heads=4,
        latent_dim_global=32,
        latent_dim_class=32,
        minority_class_id=1,
        dropout = 0.1,
        decoder_depth=2,
        gate_hidden= 64,
        align_lambda = 0.10,  # \lambda_alignï¼Œç”¨äº \mathcal{L}_{align}
        cls_lambda = 0.1,  # è‹¥ä¸å¼€ç±»åˆ«ç›‘ç£ï¼Œè¿™é‡Œä¸º 0
        kl_g_lambda = 1.00,  # å…¨å±€ KL ç³»æ•°ï¼ˆbeta-VAE é£æ ¼ï¼‰
        kl_c_lambda = 1.00,  # ç±»åˆ« KL ç³»æ•°
        recon_lambda = 1.00,  # é‡æ„é¡¹ç³»æ•°
        disentangle_lambda = 0.10,
        center_lambda = 0.0,
        cls_embed=False,
        weights = None,
        d_hid = 128,
        kernel_size = 3,
        stride = 1,
        padding =1,
        recon_metric = 'mse',
        val_data = None,
        oracle_model: nn.Module = None,
        lr = 1e-3):

        super().__init__()
        self.model = LatentGatedDualVAE(
            in_chans=in_chans,
            seq_len=seq_len,
            d_model=embed_dim,
            enc_depth=depth,
            n_heads=num_heads,
            d_hid=d_hid,
            kernel_size=kernel_size,
            stride=stride,
            padding=padding,
            latent_dim_global=latent_dim_global,
            latent_dim_class=latent_dim_class,
            minority_class_id = minority_class_id,
            dropout=dropout,
            dec_depth=decoder_depth,
            gate_hidden = gate_hidden,
            num_classes=(len(weights) if cls_embed and weights is not None else None),
            recon_metric=recon_metric
        )

        self.align_lambda = float(align_lambda)
        self.cls_lambda = float(cls_lambda)
        self.disentangle_lambda = float(disentangle_lambda)
        self.center_lambda = float(center_lambda)
        self.kl_g_lambda = float(kl_g_lambda)
        self.kl_c_lambda = float(kl_c_lambda)
        self.recon_lambda = float(recon_lambda)
        self.warmup_epochs = 5.0
        self.minority_class_id = minority_class_id
        self.input_channels = in_chans
        self.validation_step_outputs = []
        self.val_data = val_data
        self.train_data = []
        self.train_data_sample = True
        self.oracle = oracle_model
        if self.oracle:
            self.oracle.eval()
            for param in self.oracle.parameters():
                param.requires_grad = False  # ç¡®ä¿ä¸è¢«è¯¯è®­ç»ƒ
        if cls_embed and weights is not None:
            num_c = len(weights)
            print(weights)
            self.train_f1 = torchmetrics.F1Score(task="multiclass", num_classes=num_c, average="macro")
            self.valid_acc = torchmetrics.Accuracy(task="multiclass", num_classes=num_c)
            # self.valid_f1 = torchmetrics.F1Score(task="multiclass", num_classes=num_c, average="macro")
            self.valid_f1 = torchmetrics.F1Score(task="multiclass", num_classes=num_c, average=None)
        else:
            self.train_f1 = None
            self.valid_acc = None
            self.valid_f1 = None

        self.lr = lr
        self.fig = None
        self.save_hyperparameters()

    def on_train_epoch_start(self):
        print(f"Starting epoch {self.current_epoch}")
        current_epoch = self.current_epoch

        # 2. è·å– train_dataloader
        # æ³¨æ„ï¼šself.trainer.train_dataloader é€šå¸¸æ˜¯å½“å‰æ­£åœ¨ä½¿ç”¨çš„ loader
        loader = self.trainer.train_dataloader

        # 3. æ‰¾åˆ° sampler å¹¶è®¾ç½® epoch
        # æœ‰äº›æƒ…å†µä¸‹ loader å¯èƒ½è¢« Lightning åŒ…è£…è¿‡ï¼Œæ‰€ä»¥æœ€å¥½åŠ ä¸ª hasattr åˆ¤æ–­
        if hasattr(loader, "sampler") and hasattr(loader.sampler, "set_epoch"):
            loader.sampler.set_epoch(current_epoch)

            # (å¯é€‰) æ‰“å°ä¸€ä¸‹ç¡®è®¤åˆ‡æ¢çŠ¶æ€
            if current_epoch == loader.sampler.switch_epoch:
                print(f"\n[Info] Epoch {current_epoch}: Switching to FULL weighted sampling!")

    def training_step(self, batch, batch_idx):
        if len(batch) == 2:
            batch,y = batch
        else:
            y = None

        # 1) å½“å‰ epoch ä¸‹çš„æœ‰æ•ˆæƒé‡
        turn_off_dilate =False
        recon_w, kl_g_w, kl_c_w, align_w, disentangle_w, center_w, cls_w,turn_off_dilate = self._current_loss_weights()
        # === 2) æŠŠæœ‰æ•ˆæƒé‡ä¼ è¿›æ¨¡å‹ ===
        out = self.model(batch, y, turn_off_dilate=turn_off_dilate)

        recon_loss = out.get("recon_loss", 0.0)
        kl_g = out.get("kl_g", 0.0)
        kl_c = out.get("kl_c", 0.0)
        align_loss = out.get("align_loss", 0.0)
        disentangle_loss = out.get("disentangle_loss", 0.0)
        loss_center = out.get("loss_center", 0.0)
        cls_loss = out.get("cls_loss", None)

        # 3) åœ¨ PL é‡Œç»„åˆ total loss
        loss = recon_w * recon_loss + kl_g_w * kl_g + kl_c_w * kl_c
        loss = loss + align_w * align_loss
        loss = loss + disentangle_w * disentangle_loss
        loss = loss + center_w * loss_center
        if cls_loss is not None:
            loss = loss + cls_w * cls_loss

        self.log("train/align_weight_eff", align_w, prog_bar=True, sync_dist=True)
        self.log("train/disentangle_weight_eff", disentangle_w, prog_bar=True, sync_dist=True)
        self.log("train/center_weight_eff", center_w, prog_bar=True, sync_dist=True)

        # 5) æ€» loss
        self.log(
            "train/loss",
            loss,
            on_step=True,
            on_epoch=True,
            prog_bar=True,
            sync_dist=True,
        )

        # 6) å„é¡¹ raw loss
        self.log("train/recon_loss", recon_loss, on_step=False, on_epoch=True, sync_dist=True)
        self.log("train/kl_g", kl_g, on_step=False, on_epoch=True, sync_dist=True)
        self.log("train/kl_c", kl_c, on_step=False, on_epoch=True, sync_dist=True)
        self.log("train/align_loss", align_loss, on_step=False, on_epoch=True, sync_dist=True)
        self.log("train/disentangle_loss", disentangle_loss, on_step=False, on_epoch=True, sync_dist=True)
        self.log("train/loss_center", loss_center, on_step=False, on_epoch=True, sync_dist=True)

        if cls_loss is not None:
            self.log("train/cls_loss", cls_loss, on_step=False, on_epoch=True, sync_dist=True)
            if (
                    self.train_f1 is not None
                    and out.get("cls_logits") is not None
                    and y is not None
            ):
                f1_val = self.train_f1(out["cls_logits"], y)
                self.log("train/F1_step", f1_val, prog_bar=False, sync_dist=True)

        return loss

    def on_train_epoch_end(self,):
        if self.train_f1:
            self.train_f1.reset()


    def validation_step(self, batch, batch_idx):
        if len(batch) == 2:
            x, y = batch
        else:
            x, y = batch, None

        if self.train_data_sample:
            is_min = (y == self.minority_class_id)
            is_maj = ~is_min
            # å¿…é¡»ä½¿ç”¨ .detach() é¿å…è®¡ç®—å›¾ç§¯å‹
            self.train_data.append({
                "x_majority": x[is_maj].detach(),
                "x_minority": x[is_min].detach()
            })


    def on_validation_epoch_end(self):
        # 1. æ£€æŸ¥æ˜¯å¦æœ‰æ•°æ®
        if not self.train_data:
            return

        X_majority = torch.cat([d["x_majority"] for d in self.train_data], dim=0)
        X_minority = torch.cat([d["x_minority"] for d in self.train_data], dim=0)
        # å¦‚æœä½ åªæƒ³é‡‡æ ·ä¸€æ¬¡è®­ç»ƒé›†ä½œä¸º benchmarkï¼Œä¿ç•™æ­¤ flag
        # å¦‚æœå¸Œæœ›æ¯è½®éƒ½åˆ·æ–°ï¼Œå°±åœ¨æœ«å°¾æŠŠæ­¤ flag è®¾ä¸º True
        # self.train_data_sample = False

        device = X_majority.device

        # 2. åˆ’åˆ†è®­ç»ƒ/æµ‹è¯• (å¦‚æœæ²¡æœ‰å¤–éƒ¨éªŒè¯é›†)
        if self.val_data is None:
            # ... åŸæœ‰çš„åˆ‡ç‰‡é€»è¾‘ ...
            # å»ºè®®åŠ ä¸€ä¸ªä¿é™©ï¼š
            idx_min = max(1, int(X_minority.size(0) * 0.7))
            idx_maj = max(1, int(X_majority.size(0) * 0.7))
            minority_train, minority_test = X_minority[:idx_min], X_minority[idx_min:]
            majority_train, majority_test = X_majority[:idx_maj], X_majority[idx_maj:]
        else:
            minority_train = X_minority
            majority_train = X_majority
            # ç¡®ä¿ val_data ä¹Ÿåœ¨æ­£ç¡®çš„è®¾å¤‡ä¸Š
            v_x, v_y = self.val_data["x_test"].to(device), self.val_data["y_test"].to(device)
            minority_test = v_x[v_y == self.minority_class_id]
            majority_test = v_x[v_y != self.minority_class_id]

        # 3. åŒåˆ†ç±»å™¨ç­›é€‰ç”Ÿæˆ
        target_count = minority_train.size(0) * 9
        new_minority = self.get_filtered_samples(minority_train, target_count)
        # new_minority = self.model.generate_vae_prior(minority_train, num_variations=9, alpha=0.5)
        # 4. æ„å»ºè¯„ä¼°é›†
        all_x_train = torch.cat([majority_train, minority_train, new_minority], dim=0)
        all_y_train = torch.cat([
            torch.full((len(majority_train),), 0, device=device),
            torch.full((len(minority_train) + len(new_minority),), 1, device=device)
        ], dim=0)

        all_x_test = torch.cat([majority_test, minority_test], dim=0)
        all_y_test = torch.cat([
            torch.full((len(majority_test),), 0, device=device),
            torch.full((len(minority_test),), 1, device=device)
        ], dim=0)
        res_g, res_f1, res_acc = 0., 0., 0.
        res_gen = res_f1

        # 3. æ¯éš” N ä¸ª Epoch æ‰§è¡Œåˆ†ç±»å™¨è¯„ä¼°
        if self.current_epoch >= 5:
            metrics = train_and_eval_classifier(
                all_x_train, all_y_train,
                all_x_test, all_y_test,
                input_chans=self.input_channels,
                seq_len=all_x_train.shape[-1],
                device=self.device
            )
            res_g = metrics["val_g_means"]
            res_f1 = metrics["val_f1_macro"]
            res_acc = metrics["val_acc"]
            res_gen = res_f1

        # 4. ç»Ÿä¸€åœ¨åˆ†æ”¯å¤– Logï¼Œç¡®ä¿å‚æ•°æ°¸è¿œä¸€è‡´
        self.log("eval/gen_g_means", res_g, prog_bar=True)
        self.log("eval/gen_f1_macro", res_f1, prog_bar=True)
        self.log("eval/acc", res_acc, prog_bar=True)
        self.log("eval_gen", res_gen, prog_bar=True)

        # 5. æ¸…ç©ºåˆ—è¡¨
        self.train_data.clear()
        # å¦‚æœå¸Œæœ›ä¸‹ä¸€è½®ç»§ç»­æ”¶é›†ï¼Œé‡ç½® flag (å–å†³äºä½ çš„é€»è¾‘éœ€æ±‚)
        self.train_data_sample = True

    def configure_optimizers(self):
        # ç»Ÿä¸€ä» self.cfg è¯»å–ï¼ˆç”± train.py èµ‹å€¼ä¸º DotDict/ dictï¼‰
        cfg = getattr(self, "cfg", {}) or {}

        optim_cfg = cfg.get("optim", {}) or {}
        trainer_cfg = cfg.get("trainer", {}) or {}

        lr = float(optim_cfg.get("lr", 1e-3))
        wd = float(optim_cfg.get("weight_decay", 0.0))
        raw_betas = optim_cfg.get("betas", (0.9, 0.999))
        if isinstance(raw_betas, (list, tuple)) and len(raw_betas) == 2:
            betas: tuple[float, float] = (float(raw_betas[0]), float(raw_betas[1]))
        else:
            betas = (0.9, 0.999)

        opt_name = str(optim_cfg.get("optimizer", "adam")).lower()
        if opt_name == "adamw":
            opt = torch.optim.AdamW(self.parameters(), lr=lr, betas=betas, weight_decay=wd)
        else:
            opt = torch.optim.Adam(self.parameters(), lr=lr, betas=betas, weight_decay=wd)

        scheduler_name = str(optim_cfg.get("scheduler", "cosine")).lower()
        if scheduler_name == "cosine":
            cos = optim_cfg.get("cosine", {}) or {}
            T_max = int(trainer_cfg.get("max_epochs", cos.get("T_max", 10)))
            eta_min = float(cos.get("eta_min", 0.0))
            scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(opt, T_max=T_max, eta_min=eta_min)
            return [opt], [{"scheduler": scheduler, "interval": "epoch"}]

        return opt

    def _current_loss_weights(self):
        """Compute effective loss weights with epoch-based warmup."""
        epoch = float(self.current_epoch)
        factor = min(1.0, max(0.0, epoch / self.warmup_epochs))
        if epoch <= 5:
            turn_off_dilate = True
        else:
            turn_off_dilate = False
        recon_weight = self.recon_lambda

        # åªæœ‰æ­£åˆ™åŒ–é¡¹éœ€è¦ Warmup
        kl_g_weight = self.kl_g_lambda * factor
        kl_c_weight = self.kl_c_lambda * factor
        align_weight = self.align_lambda * factor
        center_weight = self.center_lambda * factor
        disentangle_weight = self.disentangle_lambda * factor

        # åˆ†ç±»æŸå¤±å§‹ç»ˆä¿æŒæ»¡é¢ï¼Œæä¾›å¼ºç›‘ç£ä¿¡å·
        cls_weight = self.cls_lambda

        return recon_weight, kl_g_weight, kl_c_weight, align_weight, disentangle_weight, center_weight, cls_weight, turn_off_dilate

    def get_filtered_samples(self, x_min, target_num, threshold=0.7):
        if self.oracle is not None:
            self.oracle.to(self.device)
            self.oracle.eval()

        num_variations = 9
        if self.oracle is None:
            return self.model.generate_vae_prior(x_min, num_variations=num_variations, alpha=0.5)

        # 1. æ‰©å¤§ç”ŸæˆèŒƒå›´ï¼ˆæ¯”å¦‚ç”Ÿæˆ 5 å€äºç›®æ ‡çš„æ ·æœ¬é‡ï¼‰
        candidate_multiplier = 5
        candidates = self.model.generate_vae_prior(
            x_min.repeat(candidate_multiplier, 1, 1),
            num_variations=num_variations,
            alpha=0.5
        )

        # 2. è®© Static Oracle è¯„ä»·è¿™äº›æ ·æœ¬
        with torch.no_grad():
            logits = self.oracle(candidates)
            probs = torch.softmax(logits, dim=1)

            # æå–å±äºå°‘æ•°ç±»ï¼ˆID=1ï¼‰çš„æ¦‚ç‡
            minority_probs = probs[:, self.minority_class_id]
            mask = (minority_probs > threshold) & (minority_probs <= 0.999)
            valid_candidates = candidates[mask]
            if len(valid_candidates) >= target_num:
                print(
                    f"Generated {len(candidates)} candidates, with valid condidates {len(valid_candidates)} based on oracle confidence.")
                valid_candidates = valid_candidates[:target_num]
            elif len(valid_candidates) == 0:
                print(f"Warning: No candidates passed the oracle filter with threshold {threshold}. Returning unfiltered samples.")
                valid_candidates = candidates[:target_num]
        print(f"Generated {len(candidates)} candidates, selected top {len(valid_candidates)} based on oracle confidence.")
        return valid_candidates

def train_and_eval_classifier(train_data, train_labels, test_data, test_labels, input_chans, seq_len, device):
    # 1. åˆ›å»ºä¸€ä¸ªä¸´æ—¶æ–‡ä»¶å¤¹ï¼Œä¸“é—¨å­˜æ”¾è¿™æ¬¡è¯„ä¼°çš„ checkpoint
    import tempfile
    from torch.utils.data import DataLoader, TensorDataset
    import os
    import shutil
    temp_dir = tempfile.mkdtemp()

    # 2. å‡†å¤‡æ•°æ®
    train_ds = TensorDataset(train_data, train_labels)
    test_ds = TensorDataset(test_data, test_labels)
    train_loader = DataLoader(train_ds, batch_size=32, shuffle=True)
    test_loader = DataLoader(test_ds, batch_size=32)

    # 3. åˆå§‹åŒ–åˆ†ç±»å™¨ (ä½¿ç”¨ TimesNetQualityClassifier)
    # è¿™é‡Œçš„å‚æ•°æ ¹æ®ä½ çš„æ•°æ®æƒ…å†µè°ƒæ•´
    clf = TimesNetQualityClassifier(
        input_channels=input_chans,
        seq_len=seq_len,
        num_classes=2,
        d_model=64,
        top_k=3,
        lr=1e-3
    ).to(device)

    # 4. é…ç½® Checkpoint è®°è´¦å‘˜
    checkpoint_callback = pl.callbacks.ModelCheckpoint(
        dirpath=temp_dir,
        filename="best_eval",
        monitor="val_loss",  # ç›‘æ§ Macro-F1 ä½œä¸ºä¸»è¦æŒ‡æ ‡
        mode="min",  # loss è¶Šå°è¶Šå¥½
        save_top_k=1,
        save_weights_only=True
    )

    # 5. è½»é‡åŒ– Trainer
    eval_trainer = pl.Trainer(
        max_epochs=25,
        accelerator="auto",
        devices=1,
        enable_checkpointing=True,  # å¿…é¡»å¼€å¯æ‰èƒ½è¿½è¸ª best_score
        logger=False,
        callbacks=[checkpoint_callback],
        enable_progress_bar=False
    )

    try:
        # å¼€å§‹è®­ç»ƒ
        eval_trainer.fit(clf, train_loader, test_loader)

        # 6. æå–å†å²æœ€é«˜åˆ†
        best_g_means = checkpoint_callback.best_model_score
        best_acc = eval_trainer.callback_metrics.get("val_acc")  # å¦‚æœä½ æƒ³æ‹¿å…¶ä»–çš„
        best_f1 = eval_trainer.callback_metrics.get("val_f1_macro")

        results = {
            "val_g_means": best_g_means.item() if best_g_means is not None else 0.0,
            "val_acc": best_acc.item() if best_acc is not None else 0.0,
            "val_f1_macro": best_f1.item() if best_f1 is not None else 0.0
        }
    finally:
        # 7. é”€æ¯ä¸´æ—¶æ–‡ä»¶å¤¹åŠå…¶æ‰€æœ‰å†…å®¹
        if os.path.exists(temp_dir):
            shutil.rmtree(temp_dir)
            # print(f"ğŸ§¹ Temporary evaluator files cleaned from {temp_dir}")

    return results
