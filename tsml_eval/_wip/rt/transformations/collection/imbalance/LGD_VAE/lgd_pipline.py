# src/pipeline/lgd_vae_pipeline.py

from __future__ import annotations

import os
import inspect
from datetime import datetime
from typing import Optional, Tuple

import yaml
import numpy as np

import torch
torch.set_float32_matmul_precision("high")

import lightning.pytorch as pl
from lightning.pytorch.callbacks import ModelCheckpoint, Callback, EarlyStopping
from lightning.pytorch.loggers import CSVLogger
from torch.utils.data import DataLoader, WeightedRandomSampler

from tsml_eval._wip.rt.transformations.collection.imbalance.LGD_VAE.src.nn.pl_model import LitAutoEncoder
from tsml_eval._wip.rt.transformations.collection.imbalance.LGD_VAE.src.data.dataset import (
    UCRDataset,
    ZScoreNormalizer,
    load_ucr_splits,
)
from tsml_eval._wip.rt.transformations.collection.imbalance.LGD_VAE.inference import Inference  # å¦‚æœä¸åœ¨åŒä¸€çº§ï¼Œæ ¹æ®ä½ çš„é¡¹ç›®ç»“æ„è°ƒæ•´è·¯å¾„


# --------------------------------------------------------
# DotDict: æ”¯æŒ cfg.xxx ç‚¹æ“ä½œ
# --------------------------------------------------------
class DotDict(dict):
    def __init__(self, *args, **kwargs):
        super().__init__()
        data = dict(*args, **kwargs)
        for k, v in data.items():
            self[k] = self._convert(v)

    @staticmethod
    def _convert(v):
        if isinstance(v, dict):
            return DotDict(v)
        if isinstance(v, list):
            return [DotDict._convert(x) for x in v]
        return v

    def __getattr__(self, name):
        try:
            return self[name]
        except KeyError:
            raise AttributeError(name)

    def __setattr__(self, name, value):
        self[name] = self._convert(value)

    def __delattr__(self, name):
        try:
            del self[name]
        except KeyError:
            raise AttributeError(name)


import torch
from torch.utils.data import Sampler
import random


class SwitchableWeightedSampler(Sampler):
    def __init__(self, full_weights, majority_indices, switch_epoch, num_samples=None, replacement=True):
        """
        Args:
            full_weights: å¯¹åº” dataset ä¸­æ¯ä¸ªæ ·æœ¬çš„æƒé‡ (ç”¨äºé˜¶æ®µäºŒ)
            majority_indices: majority æ ·æœ¬çš„ index åˆ—è¡¨ (ç”¨äºé˜¶æ®µä¸€)
            switch_epoch: ç¬¬å‡ ä¸ª epoch å¼€å§‹åˆ‡æ¢åˆ°å…¨æ ·æœ¬åŠ æƒé‡‡æ ·
            num_samples: åŠ æƒé‡‡æ ·æ—¶çš„é‡‡æ ·æ•°é‡ (é€šå¸¸ç­‰äº dataset é•¿åº¦)
            replacement: åŠ æƒé‡‡æ ·æ—¶æ˜¯å¦æ”¾å› (é€šå¸¸ä¸º True)
        """
        super().__init__()
        self.full_weights = torch.as_tensor(full_weights, dtype=torch.double)
        self.majority_indices = majority_indices
        self.switch_epoch = switch_epoch
        self.replacement = replacement

        # å¦‚æœæ²¡æœ‰æŒ‡å®š num_samplesï¼Œé»˜è®¤ç­‰äºå…¨é‡æ•°æ®é•¿åº¦
        self.num_samples = len(self.full_weights) if num_samples is None else num_samples

        self.current_epoch = 0

    def set_epoch(self, epoch):
        self.current_epoch = epoch

    def __iter__(self):
        if self.current_epoch < self.switch_epoch:
            # === é˜¶æ®µä¸€ï¼šåªè®­ç»ƒ Majority ===
            # è¿™é‡Œé€šå¸¸ä¸éœ€è¦åŠ æƒï¼Œåªéœ€è¦éšæœºæ‰“ä¹± Majority çš„æ•°æ®
            # å¦‚æœä½ å¸Œæœ› Majority å†…éƒ¨ä¹ŸåŠ æƒï¼Œé€»è¾‘ä¼šæ›´å¤æ‚ï¼Œä½†é€šå¸¸ä¸éœ€è¦
            indices = self.majority_indices[:]
            random.shuffle(indices)

            # è¿”å›è¿­ä»£å™¨
            return iter(indices)

        else:
            # === é˜¶æ®µäºŒï¼šå…¨æ ·æœ¬åŠ æƒé‡‡æ · (æ¨¡æ‹Ÿ WeightedRandomSampler) ===
            # PyTorch WeightedRandomSampler çš„åº•å±‚æ ¸å¿ƒå°±æ˜¯ torch.multinomial
            rand_tensor = torch.multinomial(
                self.full_weights,
                self.num_samples,
                self.replacement
            )
            return iter(rand_tensor.tolist())

    def __len__(self):
        # åŠ¨æ€é•¿åº¦ï¼šæ ¹æ®é˜¶æ®µè¿”å›ä¸åŒçš„é•¿åº¦
        if self.current_epoch < self.switch_epoch:
            return len(self.majority_indices)
        else:
            return self.num_samples

# --------------------------------------------------------
# å›è°ƒï¼šæ‰“å°å„é¡¹ loss
# --------------------------------------------------------
class PrintLossCallback(Callback):
    def on_train_epoch_end(self, trainer, pl_module):
        metrics = trainer.callback_metrics
        epoch = trainer.current_epoch

        recon = metrics.get("train/recon_loss")
        kl_g = metrics.get("train/kl_g")
        kl_c = metrics.get("train/kl_c")
        align = metrics.get("train/align_loss")
        cls = metrics.get("train/cls_loss")
        center = metrics.get("train_loss_center")
        disentangle = metrics.get("train/disentangle_loss")
        total = metrics.get("train_loss")

        if total is None:
            return

        msg = f"[Epoch {epoch}] Train Loss={float(total):.4f}"
        if recon is not None:       msg += f", Recon={float(recon):.4f}"
        if kl_g is not None:        msg += f", KL_g={float(kl_g):.4f}"
        if kl_c is not None:        msg += f", KL_c={float(kl_c):.4f}"
        if align is not None:       msg += f", Align={float(align):.4f}"
        if cls is not None:         msg += f", Cls={float(cls):.4f}"
        if center is not None:      msg += f", Center={float(center):.4f}"
        if disentangle is not None: msg += f", Disentangle={float(disentangle):.4f}"

        print(msg)

    def on_validation_epoch_end(self, trainer, pl_module):
        metrics = trainer.callback_metrics
        epoch = trainer.current_epoch
        gmeans = metrics.get("eval/gen_g_means")
        macrof1 = metrics.get("eval/gen_f1_macro")
        acc = metrics.get("eval/acc")
        gen_metrics = metrics.get("eval_gen")
        if acc is not None:
            print(f"[Epoch {epoch}] Val acc={float(acc):.4f}"
                  f", gmeans={float(gmeans):.4f}"
                    f", macrof1={float(macrof1):.4f}"
                    f", gen={float(gen_metrics):.4f}"
                  )


# --------------------------------------------------------
# EarlyStopping with warmup: DelayedEarlyStopping
# --------------------------------------------------------
class DelayedEarlyStopping(EarlyStopping):
    def __init__(self, warmup_epochs: int = 10, *args, **kwargs):
        self.warmup_epochs = int(warmup_epochs)
        super().__init__(*args, **kwargs)
        print(f"[DelayedEarlyStopping] Initialized. Warmup: {self.warmup_epochs} epochs. Monitoring: {self.monitor}")

    def on_validation_epoch_end(self, trainer, pl_module):
        epoch = trainer.current_epoch

        # è·å–å½“å‰ç›‘æ§æŒ‡æ ‡çš„å€¼
        logs = trainer.callback_metrics
        current_score = logs.get(self.monitor)

        # 1. Warmup é˜¶æ®µ
        if epoch < self.warmup_epochs:
            self.wait_count = 0  # å¼ºåˆ¶é‡ç½®è®¡æ•°å™¨
            if current_score is not None:
                print(f"[Epoch {epoch}] Warmup Phase: {self.monitor} = {current_score:.4f} (EarlyStopping Inactive)")
            else:
                print(f"[Epoch {epoch}] Warmup Phase: Waiting for {self.monitor}...")
            return

        # 2. æ­£å¸¸ç›‘æ§é˜¶æ®µ
        # åœ¨è°ƒç”¨çˆ¶ç±»é€»è¾‘å‰è®°å½•ä¸€ä¸‹æ—§çš„ wait_count
        old_wait_count = self.wait_count

        # è°ƒç”¨åŸºç±»é€»è¾‘æ‰§è¡ŒçœŸæ­£çš„æ£€æŸ¥
        super().on_validation_epoch_end(trainer, pl_module)

        # 3. æ‰“å° Debug ä¿¡æ¯
        if current_score is not None:
            status = "Improved" if self.wait_count == 0 else "No Improvement"
            best_score = self.best_score.item() if hasattr(self.best_score, "item") else self.best_score

            print(f"ğŸ” [Epoch {epoch}] Monitoring: {self.monitor} = {current_score:.4f} | "
                  f"Best = {best_score:.4f} | "
                  f"Patience = {self.wait_count}/{self.patience} ({status})")

            if self.wait_count >= self.patience:
                print(f"[Early Stop] Patience reached at epoch {epoch}. Stopping training...")
        else:
            print(f"[Epoch {epoch}] Warning: {self.monitor} not found in callback_metrics!")

    def on_train_end(self, trainer, pl_module):
        print("Training finished. Final EarlyStopping state: "
              f"Best {self.monitor} = {self.best_score:.4f}, total epochs = {trainer.current_epoch}")
        print("Best model path:", getattr(trainer.checkpoint_callback, "best_model_path", "N/A"))

from lightning.pytorch.callbacks import ModelCheckpoint


class DelayedModelCheckpoint(ModelCheckpoint):
    def __init__(self, warmup_epochs: int = 10, *args, **kwargs):
        super().__init__(*args, **kwargs)
        self.start_save_epoch = int(warmup_epochs)
        print(f"ğŸ’¾ [DelayedModelCheckpoint] Initialized. Saving will start after Epoch {self.start_save_epoch}")

    def on_validation_end(self, trainer, pl_module):
        epoch = trainer.current_epoch
        if epoch < self.start_save_epoch:
            # ä»…ä»…æ‰“å°ï¼Œä¸è°ƒç”¨ super()ï¼Œä»è€Œè·³è¿‡ä¿å­˜é€»è¾‘
            if trainer.is_global_zero:  # åªåœ¨ä¸»è¿›ç¨‹æ‰“å°ï¼Œé¿å…å¤šå¡è®­ç»ƒåˆ·å±
                print(f"â³ [Epoch {epoch}] Checkpoint: Warmup phase, skipping save...")
            return

        # è®°å½•ä¸€ä¸‹ä¿å­˜å‰çš„è·¯å¾„ï¼Œç”¨äº debug
        old_best = self.best_model_path

        super().on_validation_end(trainer, pl_module)

        # å¦‚æœä¿å­˜è·¯å¾„å˜äº†ï¼Œè¯´æ˜å­˜äº†æ–°æ¨¡å‹
        if self.best_model_path != old_best:
            print(f"ğŸŒŸ [Epoch {epoch}] Checkpoint: New best model saved at {self.best_model_path}")

# ========================================================
# LGD-VAE ç«¯åˆ°ç«¯ Pipeline: __init__ + fit + transform
# ========================================================
class LGDVAEPipeline:
    """
    ä¸€ä¸ªç®€å•çš„ end-to-end ç®¡çº¿ï¼š
      - __init__ è´Ÿè´£åŠ è½½/æ•´ç† YAML é…ç½®ï¼ˆç›¸å½“äºåŸæ¥ train.py çš„ argparse + cfg éƒ¨åˆ†ï¼‰
      - fit(...) è´Ÿè´£è®­ç»ƒï¼ˆç›¸å½“äºåŸæ¥ main() é‡Œå¤§éƒ¨åˆ†ä»£ç ï¼‰
      - transform(...) ç»Ÿä¸€è°ƒç”¨ Inference çš„ç”Ÿæˆæ¥å£
    """

    def __init__(
        self,
        dataset_name: str = "FiftyWords",
        seed: int | None = None,
        device: torch.device | None = None,
        task: str = "generation",  # generation / classification
    ):
        """
        dataset_name: è¦†ç›– YAML é‡Œçš„ data.dataset_name
        seed: è¦†ç›– YAML ä¸­ experiment.seed
        device: æ‰‹åŠ¨æŒ‡å®šè®­ç»ƒ/æ¨æ–­è®¾å¤‡ï¼ˆä¸æŒ‡å®šå°±äº¤ç»™ Lightning å’Œ Inference è‡ªå·±å¤„ç†
        mode: è¿è¡Œæ¨¡å¼ï¼Œclassification / generation if is classification will load pretrained model for feature extraction
        """
        # 1) è¯» YAML
        import socket
        hostname = socket.gethostname()
        is_iridis = "iridis" in hostname.lower() or "loginx" in hostname.lower()
        is_mac = "mac" in hostname.lower() or "CH-Qiu" in hostname  # ä½ çš„æœ¬æœºå
        self.is_iridis = is_iridis
        if is_iridis:
            print("[ENV] Detected Iridis HPC environment")
            cfg_path = "/home/cq2u24/tsml-eval/tsml_eval/_wip/rt/transformations/collection/imbalance/LGD_VAE/config.yaml"
        elif is_mac:
            print("[ENV] Detected local macOS environment")
            cfg_path = ("/Users/qiuchuanhang/PycharmProjects/tsml-eval/tsml_eval/_wip/rt/transformations"
                        "/collection/imbalance/LGD_VAE/local_test.yaml")
        else:
            print(f"[ENV] Unknown environment {hostname}, fallback to iridis")
            cfg_path = "/home/cq2u24/tsml-eval/tsml_eval/_wip/rt/transformations/collection/imbalance/LGD_VAE/config.yaml"

        with open(cfg_path) as f:
            raw_cfg = yaml.safe_load(f)

        # 2) è¦†ç›– data.dataset_name & experiment.seed
        raw_cfg.setdefault("data", {})
        raw_cfg["data"]["dataset_name"] = dataset_name
        print(dataset_name)
        if seed is not None:
            raw_cfg.setdefault("experiment", {})
            raw_cfg["experiment"]["seed"] = seed

        # 3) è®¾ç½®æ¯ä¸ªæ•°æ®é›†ä¸“å±çš„ ckpt ç›®å½•
        if "paths" in raw_cfg:
            ckpt_root = raw_cfg["paths"].get("ckpt_dir", "./checkpoints")
            raw_cfg["paths"]["ckpt_dir"] = os.path.join(ckpt_root, dataset_name+str(seed))

            # callbacks ä¸­ checkpointing.dirpath ä¹ŸåŒæ­¥
            raw_cfg.setdefault("callbacks", {}).setdefault("checkpointing", {})
            raw_cfg["callbacks"]["checkpointing"]["dirpath"] = raw_cfg["paths"]["ckpt_dir"]

        # è½¬ä¸º DotDictï¼Œæ–¹ä¾¿ cfg.xxx çš„å†™æ³•
        self.cfg = DotDict(raw_cfg)
        self.cfg_path = cfg_path
        self.dataset_name = dataset_name
        self.seed = seed
        self.mean_ = None
        self.std_ = None

        # 4) è®¾ç½®éšæœºç§å­
        seed_val = seed
        pl.seed_everything(seed_val, workers=True)

        # è®¾å¤‡è®°å½•ä¸€ä¸‹ï¼ˆTrainer ä¼šè‡ªå·±å¤„ç†ï¼ŒInference ä¹Ÿä¼šç”¨åˆ°ï¼‰
        self.device = device

        # è®­ç»ƒå¥½åçš„å¯¹è±¡
        self.trainer: pl.Trainer | None = None
        self.model: LitAutoEncoder | None = None
        self.infer: Inference | None = None
        self.task = task.lower()

    # -------------------------
    # å†…éƒ¨å·¥å…·ï¼šæ„å»º DataLoader
    # -------------------------
    def _build_dataloaders(
        self,
        X_tr: np.ndarray,
        y_tr: np.ndarray,
        X_te: np.ndarray | None = None,
        y_te: np.ndarray | None = None,
    ) -> tuple[UCRDataset, UCRDataset, DataLoader, DataLoader]:
        cfg = self.cfg

        # æ„å»º Datasetï¼ˆæ³¨æ„ï¼šæ­¤æ—¶ data å·²ç»æ˜¯ zscore åçš„ï¼Œæ‰€ä»¥ normalizer=Noneï¼‰
        _,C,L = X_tr.shape
        train_dataset = UCRDataset(
            data=X_tr,
            labels=y_tr,
            split="train",
            normalizer=None,
            augmentation_ratio=0.0,
            rebalance=True,
        )
        if X_te is not None:
            eval_dataset = UCRDataset(
                data=X_te,
                labels=y_te,
                split="test",
                normalizer=None,
                augmentation_ratio=0.0,
            )
        else:
            eval_dataset = UCRDataset(
            data=X_tr,
            labels=y_tr,
            split="eval",
            normalizer=None,
            augmentation_ratio=0.0,
            rebalance=False,
        )

        print(
            f"{datetime.now()} : Train size: {len(train_dataset)}; "
            f"Eval size: {len(eval_dataset)if eval_dataset else None}."
            f"Series Channel and length: {C}, {L}"
        )

        if getattr(train_dataset, "sample_weights", None) is not None:
            print("Use WeightedRandomSampler for training data.")
            train_loader = DataLoader(
                train_dataset,
                batch_size=cfg.data.train_batch_size,
                shuffle=False,
                num_workers=cfg.data.loader_workers,
            )
        else:
            train_loader = DataLoader(
                train_dataset,
                batch_size=cfg.data.train_batch_size,
                shuffle=True,
                num_workers=cfg.data.loader_workers,
            )
        if eval_dataset:
            eval_loader = DataLoader(
                eval_dataset,
                batch_size=len(eval_dataset),
                shuffle=False,
                num_workers=cfg.data.loader_workers,
            )
        else:
            eval_loader = None
        return train_dataset, eval_dataset, train_loader, eval_loader

    # -------------------------
    # å†…éƒ¨å·¥å…·ï¼šæ„å»º pl_model + Trainer
    # -------------------------
    def _build_model_and_trainer(self, train_dataset: UCRDataset, oracle: torch.nn.Module = None) -> tuple[LitAutoEncoder, pl.Trainer]:
        cfg = self.cfg
        train_size = len(train_dataset)
        seq_len = int(train_dataset.data.shape[-1])
        cfg_model = dict(cfg.model)  # æ‹·ä¸€ä»½ï¼Œé¿å…ä¿®æ”¹åŸé…ç½®

        # ç®€å•æŒ‰æ ·æœ¬æ•°è°ƒä¸€è°ƒæ¨¡å‹å°ºå¯¸ï¼ˆè·Ÿä½ åŸæ¥çš„é€»è¾‘ä¸€è‡´ï¼‰
        # LightningModule çš„ __init__ å‚æ•°è¿‡æ»¤ï¼ˆé˜²æ­¢å¤šä½™é”®æŠ¥é”™ï¼‰
        sig = inspect.signature(LitAutoEncoder.__init__)
        valid_keys = {
            p.name
            for p in sig.parameters.values()
            if p.kind in (
                inspect.Parameter.POSITIONAL_OR_KEYWORD,
                inspect.Parameter.KEYWORD_ONLY,
            )
        }

        # è¡¥å…… in_chans / seq_len
        if "in_chans" in valid_keys and "in_chans" not in cfg_model:
            cfg_model["in_chans"] = int(train_dataset.data.shape[1])
        if "seq_len" in valid_keys and "seq_len" not in cfg_model:
            cfg_model["seq_len"] = seq_len

        # åªä¿ç•™ pl_model éœ€è¦çš„å­—æ®µ
        model_kwargs = {k: v for k, v in cfg_model.items() if k in valid_keys}

        # æ„å»º pl_modelï¼Œæ³¨æ„æŠŠ class_freq ä¼ è¿›å»
        autoencoder = LitAutoEncoder(
            **model_kwargs,
            weights=getattr(train_dataset, "class_freq", None),
            oracle_model=oracle,  # <--- å…³é”®ä¿®æ”¹ï¼šæ³¨å…¥è£åˆ¤
            mean_=self.mean_,
            std_=self.std_,
        )
        # æŠŠå®Œæ•´ cfg æŒ‚ä¸Šï¼Œæ–¹ä¾¿ pl_model é‡Œè¯»å– optim/trainer é…ç½®
        autoencoder.cfg = cfg

        # callbacks
        callbacks = []

        # ç­–ç•¥ B: ä¿å­˜â€œé‡æ„è´¨é‡æœ€é«˜â€çš„æ¨¡å‹ (Fidelity Best)
        # è¿™ä¸ªæ¨¡å‹ç”Ÿæˆçš„æ³¢å½¢æœ€å¹³æ»‘ï¼Œæœ€åƒçœŸå®æ•°æ®
        callbacks.append(DelayedModelCheckpoint(
            dirpath=cfg.paths.ckpt_dir,
            filename="{epoch:02d}-{eval_gen:.4f}",
            monitor="eval_gen",
            mode="max",
            save_top_k=3,
            warmup_epochs=5,
            save_last=False
        ))
        callbacks.append(DelayedModelCheckpoint(
            dirpath=cfg.paths.ckpt_dir,
            filename="{epoch:02d}-{train_loss:.4f}",
            monitor="train_loss",
            mode="min",
            save_top_k=3,
            warmup_epochs=5,
            save_last=False
        ))

        if "callbacks" in cfg and "early_stopping" in cfg.callbacks:
            # allow an optional `warmup_epochs` key in the early_stopping config
            # so we can ignore early-stopping checks for the initial training epochs
            callbacks.append(DelayedEarlyStopping(**cfg.callbacks.early_stopping))
        callbacks.append(PrintLossCallback())
        import time
        run_suffix = str(int(time.time()))
        # logger: æ¯ä¸ª dataset_name ä¸€ä¸ªå­ç›®å½•ï¼Œç‰ˆæœ¬ç”¨ seed
        logger = CSVLogger(
            save_dir=cfg.paths.logs_dir,
            name=self.dataset_name+str(self.seed),
            version=f'{cfg.experiment.seed}_{run_suffix}',
        )

        trainer = pl.Trainer(
            callbacks=callbacks,
            logger=logger,
            enable_progress_bar=False,
            **(cfg.trainer if "trainer" in cfg else {}),
        )

        return autoencoder, trainer

    # åœ¨ LGDVAEPipeline ç±»ä¸­æ·»åŠ 
    def _train_static_oracle(self, X_tr, y_tr, seq_len, in_chans):
        """åœ¨ VAE è®­ç»ƒå‰ï¼Œè®­ç»ƒä¸€ä¸ªé™æ€åˆ¤åˆ«å™¨ä½œä¸º Oracleï¼Œå¹¶é€‰æ‹©éªŒè¯é›† Macro F1 æœ€å¥½çš„ç‰ˆæœ¬"""
        print("ğŸš€ Training static Oracle (discriminator) with Val-split and Macro-F1 monitoring...")

        from sklearn.model_selection import train_test_split
        from torch.utils.data import DataLoader, TensorDataset
        import lightning.pytorch as pl
        from lightning.pytorch.callbacks import ModelCheckpoint
        import tempfile
        import shutil
        import os

        # 1. åˆ’åˆ†è®­ç»ƒé›†å’ŒéªŒè¯é›† (ä¾‹å¦‚ 8:2)ï¼Œä½¿ç”¨ stratify ä¿è¯ç±»åˆ«ä¸å¹³è¡¡æ¯”ä¾‹ä¸€è‡´
        X_train, X_val, y_train, y_val = train_test_split(
            X_tr, y_tr, test_size=0.2, random_state=42, stratify=y_tr
        )

        train_loader = DataLoader(
            TensorDataset(torch.from_numpy(X_train).float(), torch.from_numpy(y_train).long()),
            batch_size=32, shuffle=True
        )
        val_loader = DataLoader(
            TensorDataset(torch.from_numpy(X_val).float(), torch.from_numpy(y_val).long()),
            batch_size=32, shuffle=False
        )

        # 2. å®ä¾‹åŒ– TimesNet (ç¡®ä¿ TimesNetQualityClassifier å†…éƒ¨æœ‰ self.log("val_f1_macro", ...))
        from tsml_eval._wip.rt.transformations.collection.imbalance.LGD_VAE.src.nn.pl_model import \
            TimesNetQualityClassifier

        oracle = TimesNetQualityClassifier(
            input_channels=in_chans,
            seq_len=seq_len,
            num_classes=len(np.unique(y_tr)),
            d_model=64,
            lr=1e-3
        )

        # 3. è®¾ç½®ä¸´æ—¶è·¯å¾„ç”¨äºä¿å­˜æœ€ä¼˜æ¨¡å‹æƒé‡
        tmp_ckpt_dir = tempfile.mkdtemp()
        checkpoint_callback = ModelCheckpoint(
            dirpath=tmp_ckpt_dir,
            filename="best_oracle",
            monitor="val_f1_macro",
            mode="max",
            save_top_k=1,
            save_weights_only=True
        )

        # 4. è®­ç»ƒ
        trainer = pl.Trainer(
            max_epochs=50,
            accelerator="auto",
            devices=1,
            callbacks=[checkpoint_callback],
            logger=False,  # å¦‚æœéœ€è¦å¯è§†åŒ–å¯ä»¥å¼€å¯ TensorBoard
            enable_progress_bar=False,
            enable_checkpointing=True  # å¿…é¡»å¼€å¯ä»¥ä½¿ç”¨ callback
        )

        trainer.fit(oracle, train_loader, val_loader)

        # 5. åŠ è½½è¡¨ç°æœ€å¥½çš„æƒé‡
        best_model_path = checkpoint_callback.best_model_path
        if best_model_path and os.path.exists(best_model_path):
            print(
                f"âœ¨ Loading best Oracle weights from {best_model_path} (Score: {checkpoint_callback.best_model_score:.4f})")
            # ç›´æ¥åŠ è½½åˆ°å½“å‰ oracle å®ä¾‹
            ckpt = torch.load(best_model_path)
            oracle.load_state_dict(ckpt['state_dict'])

        # 6. å†»ç»“å¹¶æ¸…ç†
        oracle.eval()
        for param in oracle.parameters():
            param.requires_grad = False

        # æ¸…ç†ä¸´æ—¶æ–‡ä»¶
        shutil.rmtree(tmp_ckpt_dir)

        print('âœ… Static Oracle training and selection completed.')
        return oracle

    def _tournament_selection(self, ckpt_paths, X_tr, y_tr, k_folds=5):
        from sklearn.model_selection import StratifiedKFold
        import torch
        import numpy as np
        from tsml_eval._wip.rt.transformations.collection.imbalance.LGD_VAE.src.nn.pl_model import \
            train_and_eval_classifier
        import gc
        if self.mean_ is not None and self.std_ is not None:
            X_tr = X_tr * self.std_ + self.mean_
        skf = StratifiedKFold(n_splits=k_folds, shuffle=True, random_state=42)
        ckpt_scores = {}

        for path in ckpt_paths:
            print(f"ğŸ§ Evaluating CKPT: {os.path.basename(path)}")
            temp_vae = Inference.from_checkpoint(path, LitAutoEncoder, device=self.device)

            fold_f1s = []

            for fold, (train_idx, val_idx) in enumerate(skf.split(X_tr, y_tr)):
                # 1. æå–è®­ç»ƒæŠ˜å¹¶è½¬ä¸ºåŸå§‹é‡çº§ (ä½¿ç”¨ .copy() ç¡®ä¿å®‰å…¨)
                x_train_fold = X_tr[train_idx]
                y_train_fold = y_tr[train_idx]

                # 2. å‡†å¤‡ç”Ÿæˆç”¨çš„è¾“å…¥ (å¿…é¡»æ˜¯å½’ä¸€åŒ–åçš„ï¼Œå› ä¸º VAE Encoder è§è¿‡çš„æ˜¯å½’ä¸€åŒ–åˆ†å¸ƒ)
                x_min_raw = torch.from_numpy(
                    x_train_fold[y_train_fold == self.cfg.model.minority_class_id]).float().to(self.device)

                # 3. ç”Ÿæˆ 9 å€æ–°æ ·æœ¬ (Inference ä¼šè‡ªåŠ¨åå½’ä¸€åŒ–å›åŸå§‹é‡çº§)
                with torch.no_grad():
                    # æ³¨æ„ï¼šå¦‚æœ Inference æ¥å£æ²¡å†™ num_variationsï¼Œè¯·ç¡®ä¿å†…éƒ¨ repeat é€»è¾‘
                    num_min = x_min_raw.size(0)
                    indexs = torch.randint(0, num_min, (num_min * 9,), device=self.device)
                    x_min_raw_expanded = x_min_raw[indexs]
                    x_gen = temp_vae.generate_vae_prior(x_min_raw_expanded, alpha=0.5)
                    print('   Generated samples shape:', x_gen.shape)
                # 4. æå–å¹¶è½¬æ¢éªŒè¯æŠ˜é‡çº§
                x_val_raw = X_tr[val_idx]
                y_val_fold = y_tr[val_idx]

                # 5. æ„å»ºç»Ÿä¸€åŸå§‹é‡çº§çš„å¢å¼ºè®­ç»ƒé›†
                # å°† numpy çš„åŸå§‹è®­ç»ƒæŠ˜è½¬ä¸º tensor å¹¶ç§»è‡³è®¾å¤‡
                x_train_tensor = torch.from_numpy(x_train_fold).float().to(self.device)
                x_combined = torch.cat([x_train_tensor, x_gen], dim=0)

                y_gen = torch.full((x_gen.size(0),), self.cfg.model.minority_class_id, device=self.device)
                y_combined = torch.cat([torch.from_numpy(y_train_fold).long().to(self.device), y_gen], dim=0)

                # 6. è®­ç»ƒä¸è¯„ä¼°
                metrics = train_and_eval_classifier(
                    x_combined, y_combined,
                    torch.from_numpy(x_val_raw).float().to(self.device),
                    torch.from_numpy(y_val_fold).long().to(self.device),
                    input_chans=self.cfg.model.in_chans,
                    seq_len=X_tr.shape[-1],
                    device=self.device
                )
                print(f'   Fold {fold + 1}/{k_folds} - Metrics: {metrics}')
                fold_f1s.append(metrics["val_f1_macro"])

            avg_f1 = np.mean(fold_f1s)
            ckpt_scores[path] = avg_f1
            print(f"   -> Avg Macro-F1: {avg_f1:.4f}")

            # æ˜¾å­˜æ¸…ç†
            del temp_vae
            torch.cuda.empty_cache()
            gc.collect()

        return max(ckpt_scores, key=ckpt_scores.get)
    # -------------------------
    # å…¬å…±æ¥å£ï¼šfit
    # -------------------------
    def fit(
        self,
        X_tr: np.ndarray,
        y_tr: np.ndarray,
        X_te: np.ndarray | None = None,
        y_te: np.ndarray | None = None,
    ) -> LGDVAEPipeline:
        """
        è®­ç»ƒ LGD-VAE.

        å¦‚æœä¸ä¼  X_tr/y_tr/X_te/y_teï¼Œåˆ™ä¼šè‡ªåŠ¨æŒ‰ç…§ cfg ä» UCR ç›®å½•è¯»å–ï¼›
        å¦‚æœä½ å·²ç»åœ¨å¤–é¢è‡ªå·±åšå¥½äº† splitï¼Œå°±å¯ä»¥æŠŠ numpy æ•°ç»„ç›´æ¥ä¼ è¿›æ¥ã€‚
        """
        if X_te is None:
            print("use train data for evaluation since test data is not provided.")
            X_te = X_tr
            y_te = y_tr
        cfg = self.cfg
        dataset_name = cfg.data.dataset_name
        # X_train_maj = X_tr[y_tr != cfg.model.minority_class_id]
        # X_train_min = X_tr[y_tr == cfg.model.minority_class_id]
        means = X_tr.mean(axis=2, keepdims=True)  # å¾—åˆ° (n_sample, 1, 1)
        stds = X_tr.std(axis=2, keepdims=True)  # å¾—åˆ° (n_sample, 1, 1)
        X_norm = (X_tr - means) / (stds + 1e-8)

        print(f"X_norm shape: {X_norm.shape}")
        # éªŒè¯ï¼šéšæœºé€‰ä¸€ä¸ªæ ·æœ¬çœ‹å‡å€¼æ˜¯å¦ä¸º 0
        print(f"Sample 0 mean: {X_norm[0, 0, :].mean():.4f}")
        # normalizer_maj = ZScoreNormalizer().fit(X_train_maj)
        # self.normalizer = ZScoreNormalizer().fit(X_train_min)
        # print(f"dataset maj mean: {normalizer_maj.mean_} and std: {normalizer_maj.std_}")
        # stats_dir = os.path.join(cfg.paths.work_root, "stats")
        # os.makedirs(stats_dir, exist_ok=True)
        # np.savez(os.path.join(stats_dir, f"{dataset_name}_zscore.npz"),
        #          mean=self.normalizer.mean_, std=self.normalizer.std_)

        # print(f"dataset mean: {self.normalizer.mean_} and std: {self.normalizer.std_}")
        # X_tr[y_tr != cfg.model.minority_class_id] = normalizer_maj.transform(X_tr[y_tr != cfg.model.minority_class_id])
        # X_tr[y_tr == cfg.model.minority_class_id] = self.normalizer.transform(X_tr[y_tr == cfg.model.minority_class_id])

        X_tr = X_norm
        minority_mask = (y_tr == cfg.model.minority_class_id)
        self.mean_ = means
        self.std_ = stds

        # self.mean_ = self.normalizer.mean_
        # self.std_ = self.normalizer.std_

        # prerebalance use smote

        # 2) DataLoader + Dataset
        train_dataset, eval_dataset, train_loader, eval_loader = self._build_dataloaders(
            X_tr, y_tr, X_te, y_te
        )

        # 3) æ„å»ºæ¨¡å‹ & Trainer
        # ------------------ æ–°å¢ï¼šOracle é¢„è®­ç»ƒ ------------------
        seq_len = X_tr.shape[-1]
        in_chans = X_tr.shape[1]
        # æˆ‘ä»¬åœ¨è¿™é‡Œå…ˆç»ƒä¸€ä¸ª Oracle
        # self.static_oracle = self._train_static_oracle(X_tr, y_tr, seq_len, in_chans)
        # -------------------------------------------------------
        # 3. æ„å»ºæ¨¡å‹ (ä¼ å…¥ Oracle)
        # autoencoder, trainer = self._build_model_and_trainer(train_dataset, oracle=self.static_oracle)
        autoencoder, trainer = self._build_model_and_trainer(train_dataset)

        # 4) è®­ç»ƒï¼šå¦‚æœå·²æœ‰ checkpoint åˆ™ä¼˜å…ˆå°è¯•åŠ è½½ï¼›åŠ è½½å¤±è´¥å†å›é€€åˆ°é‡æ–°è®­ç»ƒ
        ckpt_path = None
        ckpt_dir = getattr(self.cfg.paths, "ckpt_dir", None)
        env_ckpt_path = os.environ.get("LGD_VAE_CHECKPOINT_PATH")

        # ---------------- Step 1: ç¡®å®š ckpt_path è·¯å¾„ ----------------
        if env_ckpt_path and os.path.exists(env_ckpt_path):
            print(f"[Experiment] Loading specific checkpoint from env: {env_ckpt_path}")
            ckpt_path = env_ckpt_path
        else:
            # åªæœ‰åœ¨æ²¡æœ‰ç¯å¢ƒå˜é‡æŒ‡å®šæ—¶ï¼Œæ‰å»ç›®å½•è‡ªåŠ¨æœç´¢
            if ckpt_dir is not None and os.path.isdir(ckpt_dir):
                # print(f"loading checkpoint from {ckpt_dir}")
                ckpt_candidates = [f for f in os.listdir(ckpt_dir) if f.endswith(".ckpt")]

                if ckpt_candidates:
                    print(f"[Tournament] Found {len(ckpt_candidates)} checkpoints. Starting selection...")

                    # æ„å»ºå®Œæ•´çš„å€™é€‰è·¯å¾„åˆ—è¡¨
                    full_ckpt_paths = [os.path.join(ckpt_dir, f) for f in ckpt_candidates]

                    # è°ƒç”¨é€‰æ‹”èµ›é€»è¾‘ï¼ˆè§ä¸‹æ–‡å®ç°ï¼‰
                    # ä¼ å…¥åŸå§‹å½’ä¸€åŒ–åçš„æ•°æ® X_tr, y_tr
                    ckpt_path = self._tournament_selection(full_ckpt_paths, X_tr, y_tr, k_folds=5)

                    print(f"ğŸ¥‡ Tournament winner selected: {os.path.basename(ckpt_path)}")
                    # print(f"[Experiment] No env var found, auto-loading best model from {ckpt_dir}...")
                    # import re
                    # def extract_loss(name: str):
                    #     match = re.search(r"eval_gen=([0-9]+\.?[0-9]*)", name)
                    #     if match:
                    #         return float(match.group(1))
                    #     return float('inf')
                    #
                    # # è¿‡æ»¤å‡ºåŒ…å« eval/gen_f1_macro çš„æ–‡ä»¶
                    # eval_ckpts = [f for f in ckpt_candidates if "eval_gen=" in f]
                    #
                    # if eval_ckpts:
                    #     # ä½¿ç”¨ max() æ‰¾åˆ° eval/gen_f1_macro æœ€å¤§çš„æ–‡ä»¶
                    #     best_ckpt_file = max(eval_ckpts, key=extract_loss)
                    #     ckpt_path = os.path.join(ckpt_dir, best_ckpt_file)
                    #     print(f"Loading best model: {best_ckpt_file}")
                    # else:
                    #     # å…¶æ¬¡ï¼šfallback ç”¨ last.ckptï¼ˆå¦‚æœå­˜åœ¨ï¼‰
                    #     last_ckpts = [f for f in ckpt_candidates if "last" in f]
                    #     if last_ckpts:
                    #         ckpt_path = os.path.join(ckpt_dir, last_ckpts[0])
                    #     else:
                    #         # æœ€åï¼šéšä¾¿é€‰ä¸€ä¸ª
                    #         ckpt_path = os.path.join(ckpt_dir, sorted(ckpt_candidates)[0])
                else:
                    print(f"[Experiment] Checkpoint directory exists but is empty.")

        # ---------------- Step 2: å°è¯•åŠ è½½ ----------------
        if ckpt_path is not None:  # <--- [å…³é”®ä¿®æ”¹] åªæœ‰æ‰¾åˆ°äº†è·¯å¾„æ‰å°è¯•åŠ è½½
            try:
                print(f"[LGDVAEPipeline] Attempting to load checkpoint: {ckpt_path}")

                self.infer = Inference.from_checkpoint(
                    ckpt_path,
                    model_class=LitAutoEncoder,
                    model_kwargs=None,
                    device=self.device,
                    strict=False,
                )
                print("load successfully!")
                return self

            except Exception as e:
                print(f"[LGDVAEPipeline] Failed to load checkpoint ({e}).")
                print("[LGDVAEPipeline] Fallback to training from scratch.")
                ckpt_path = None  # ç¡®ä¿ä¼ ç»™ trainer çš„æ˜¯ Noneï¼Œè®©å®ƒä»å¤´å¼€å§‹
        else:
            print("[LGDVAEPipeline] No checkpoint found. Starting training from scratch.")

        # ... ä»£ç ç»§ç»­æ‰§è¡Œï¼Œè¿›å…¥ trainer.fit() ...
        if eval_dataset is not None:
            trainer.fit(
                autoencoder,
                train_loader,
                eval_loader,
                ckpt_path=ckpt_path,
            )
        else:
            trainer.fit(
                autoencoder,
                train_loader,
                ckpt_path=ckpt_path,
            )

        # 5) ä¿å­˜åˆ° pipeline æˆå‘˜é‡Œ
        self.trainer = trainer
        self.model = autoencoder

        # --- æ–°å¢ï¼šåŸºäºåˆ†ç±»é€‰æ‹”èµ›çš„æœ€ä¼˜æ¨¡å‹é€‰æ‹© ---
        print("Starting Tournament to select the best CKPT based on classification performance...")

        # 1. è·å– Top-3 CKPT è·¯å¾„
        checkpoint_callback = trainer.checkpoint_callback
        ckpt_dir = checkpoint_callback.dirpath
        best_k_models = checkpoint_callback.best_k_models  # dict: {path: score}
        ckpt_paths = list(best_k_models.keys())

        # 3. åœ¨è¯¥ç›®å½•ä¸‹æœç´¢æ‰€æœ‰ç¬¦åˆå‘½åçš„ ckpt æ–‡ä»¶
        # è¿™æ ·å¯ä»¥æåˆ°ç”±å…¶ä»– callbackï¼ˆæ¯”å¦‚ç›‘æ§ train_loss çš„ï¼‰ä¿å­˜çš„æ–‡ä»¶
        import glob
        if ckpt_dir and os.path.exists(ckpt_dir):
            search_pattern = os.path.join(ckpt_dir, "epoch=*.ckpt")
            all_local_ckpts = glob.glob(search_pattern)

            # åˆå¹¶å¹¶å»é‡ï¼ˆç»Ÿä¸€è½¬ä¸ºç»å¯¹è·¯å¾„é˜²æ­¢é‡å¤ï¼‰
            # ä½¿ç”¨ set ç¡®ä¿å³ä¾¿ best_k é‡Œçš„æ–‡ä»¶åœ¨æ–‡ä»¶å¤¹é‡Œè¢«æœåˆ°ï¼Œä¹Ÿåªä¿ç•™ä¸€ä»½
            all_ckpt_paths = list(set([os.path.abspath(p) for p in ckpt_paths] +
                                      [os.path.abspath(p) for p in all_local_ckpts]))
        else:
            print(f"Warning: Directory {ckpt_dir} not found. Using only callback records.")
            all_ckpt_paths = ckpt_paths

        # 4. æ‰“å°ç»“æœ
        print(f'Total CKPTs found in {ckpt_dir}: {len(all_ckpt_paths)}')
        for path in all_ckpt_paths:
            print(f' - {os.path.basename(path)}')
        best_path = self._tournament_selection(all_ckpt_paths, X_tr, y_tr)

        # 3. åŠ è½½æœ€ç»ˆèƒœå‡ºçš„æ¨¡å‹
        print(f"ğŸ¥‡ Tournament winner: {best_path}")
        self.infer = Inference.from_checkpoint(
            best_path,
            model_class=LitAutoEncoder,
            model_kwargs=None,
            device=self.device,
            strict=False,
        )
        return self


    # -------------------------
    # å…¬å…±æ¥å£ï¼štransform / ç”Ÿæˆ
    # -------------------------
    # def transform(self, mode: str, **kwargs) -> torch.Tensor:
    #     """
    #     ç»Ÿä¸€çš„ç”Ÿæˆæ¥å£ï¼Œå†…éƒ¨ç›´æ¥è°ƒç”¨ Inference çš„æ–¹æ³•ã€‚
    #
    #     mode:
    #       - "vae_prior"      â†’ å…ˆéªŒé‡‡æ ·ç”Ÿæˆï¼ˆgenerate_vae_priorï¼‰
    #       - "mix_pair"       â†’ minority + majority pair é—¨æ§æ··åˆï¼ˆgenerate_mix_pairï¼‰
    #       - "smote_latent"   â†’ latent ç©ºé—´ SMOTE æ’å€¼ç”Ÿæˆï¼ˆgenerate_smote_latentï¼‰
    #       - "prototype"      â†’ åŸºäº prototype ç”Ÿæˆï¼ˆgenerate_from_prototypeï¼‰
    #     """
    #     if self.infer is None:
    #         raise RuntimeError("Pipeline is not fitted yet. Call `fit()` first.")
    #
    #     mode = mode.lower()
    #     if mode == "classification" and self.task=="classification":
    #         # kwargs: x=...
    #         synthetics = self.infer.feature_extract(**kwargs)
    #     if mode == "prior":
    #         # kwargs: batch_size=..., device=...
    #         synthetics = self.infer.generate_vae_prior(**kwargs)
    #     elif mode == "pair":
    #         # kwargs: x_min=..., x_maj=..., use_y=...
    #         synthetics = self.infer.generate_mix_pair(**kwargs)
    #     elif mode == "latent":
    #         # kwargs: x_min1=..., x_min2=..., alpha=..., num_samples=...
    #         synthetics = self.infer.generate_smote_latent(**kwargs)
    #     elif mode == "prototype":
    #         # kwargs: x_min=..., use_y=...
    #         synthetics = self.infer.generate_from_prototype(**kwargs)
    #     else:
    #         raise ValueError(f"Unknown transform mode: {mode!r}")
    #
    #     return synthetics

    def transform(self, x_min, mode: str=None, threshold=0.7, max_retries=10, alpha = None):
        """
        å¸¦æœ‰æ‹’ç»é‡‡æ ·çš„å°‘æ•°ç±»ç”Ÿæˆ
        x_min: åŸå§‹å°‘æ•°ç±»æ ·æœ¬ [B, C, T]
        threshold: åˆ¤åˆ«å™¨ä¿¡å¿ƒé˜ˆå€¼ï¼Œå»ºè®® 0.8 ä»¥ä¸Š
        max_retries: å°è¯•ç”Ÿæˆçš„æœ€å¤§è½®æ¬¡ï¼Œé˜²æ­¢æ­»å¾ªç¯
        """
        self.infer.model.eval()
        # å‡è®¾ä½ å·²ç»åŠ è½½äº†ä¸€ä¸ªé¢„è®­ç»ƒå¥½çš„åˆ†ç±»å™¨åˆ° self.discriminator
        # å¦‚æœæ²¡æœ‰ï¼Œå¯ä»¥ä½¿ç”¨ pl_model.py ä¸­çš„ TimesNetQualityClassifier
        all_needed = x_min.size(0)
        device = x_min.device
        discriminator = getattr(self, "static_oracle", None)

        if discriminator is None:
            print("âš ï¸ Warning: No static oracle found, performing standard generation.")
            return self.infer.generate_vae_prior(x_min=x_min, alpha=0.5)

        discriminator.to(x_min.device)
        batch_size = x_min.size(0)
        final_samples = []

        # æˆ‘ä»¬å¾ªç¯å°è¯•ï¼Œç›´åˆ°æ”¶é›†å¤Ÿ batch_size æ•°é‡çš„é«˜è´¨é‡æ ·æœ¬

        for i in range(max_retries):
            current_needed = batch_size - sum(len(s) for s in final_samples)
            if current_needed <= 0:
                break

            # 1. è°ƒç”¨ä½ ç°æœ‰çš„ç”Ÿæˆé€»è¾‘ (Alpha=0.5 æ’å€¼)
            # å¯¹åº” model.py ä¸­çš„ generate_vae_prior å®ç°
            candidates = self.infer.generate_vae_prior(x_min=x_min, alpha=0.5)

            # 2. åˆ¤åˆ«å™¨è¯„ä¼° (ç¡®ä¿è¾“å…¥ç»´åº¦æ­£ç¡® [B, C, T])
            with torch.no_grad():
                # è¿™é‡Œçš„ discriminator éœ€é¢„å…ˆè®­ç»ƒå¹¶åŠ è½½
                logits = discriminator(candidates)
                probs = torch.nn.functional.softmax(logits, dim=1)

            # 3. ç­›é€‰å°‘æ•°ç±» (ID=1) ä¸”ä¿¡å¿ƒå€¼è¾¾æ ‡çš„æ ·æœ¬
            minority_id = self.cfg.model.minority_class_id
            conf = probs[:, minority_id]
            mask = (conf > threshold) & (conf <= 0.99)

            valid_candidates = candidates[mask]
            if len(valid_candidates) > 0:
                final_samples.append(valid_candidates)

        # 4. æ±‡æ€»ç»“æœ
        if not final_samples:
            # å¦‚æœå¤šæ¬¡å°è¯•éƒ½å¤±è´¥ï¼Œå›é€€åˆ°åŸå§‹ç”Ÿæˆæˆ–æŠ¥é”™
            print("[LGDVAEPipeline] No valid samples found.")
            return self.infer.generate_vae_prior(x_min=x_min)

        all_generated = torch.cat(final_samples, dim=0)
        if len(all_generated) <= all_needed:
            n_needed = all_needed - len(all_generated)
            print(f"[LGDVAEPipeline] Only {len(all_generated)} valid samples found, need {n_needed} more. Generating additional samples without filtering...")
            additional = self.infer.generate_vae_prior(x_min=x_min[:n_needed], alpha=0.5)
            all_generated = torch.cat([all_generated, additional], dim=0)
        return all_generated[:batch_size]
