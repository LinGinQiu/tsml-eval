import json
import os
import subprocess
import sys

# ================= ğŸš€ ç”¨æˆ·é…ç½®åŒºåŸŸ =================

# 1. ç”¨æˆ·ä¿¡æ¯
USERNAME = "cq2u24"

# 2. æ•°æ®é›†åˆ—è¡¨æ–‡ä»¶è·¯å¾„
DATASET_LIST_FILE = f"/scratch/{USERNAME}/DataSetLists/Longset2-2.txt"

# 3. åˆ†ç±»å™¨å›ºå®šä¸º TimesNet
CLASSIFIER = "TimesNet"

# 4. Checkpoint çš„ç§å­åç¼€ (åŒæ—¶ä¹Ÿæ˜¯ Resample ID)
SEED_SUFFIX = "0"

# 5. è·¯å¾„é…ç½®
# TimesNet ä¸“ç”¨çš„è¿è¡Œè„šæœ¬
SCRIPT_PATH = f"/home/{USERNAME}/tsml-eval/TimesNet/run.py"
# TimesNet çš„ Config æ–‡ä»¶ç›®å½•
CONFIG_DIR = f"/home/{USERNAME}/tsml-eval/TimesNet/TSTConfig/"

DATA_DIR_BASE = f"/scratch/{USERNAME}/Data/imbalanced_9_1/"
BASE_RESULTS_ROOT = f"/scratch/{USERNAME}/Exp/results_TimesNet_generative/"
CKPT_ROOT = f"/scratch/{USERNAME}/models/MGD_CVAE/checkpoints/"

# ================= âš™ï¸ ç¯å¢ƒä¸èµ„æºé…ç½® =================

# TimesNet å¿…é¡»è·‘åœ¨ DL ç¯å¢ƒ
ENV_DL = f"/scratch/{USERNAME}/conda-envs/deeplearning_tf"

# é˜Ÿåˆ— (TimesNet å¿…é¡»ç”¨ GPU)
QUEUE_GPU = "a100,swarm_a100,swarm_h100,l4,swarm_l4"

# èµ„æºé™åˆ¶
MEM = "32000M"
TIME = "10:00:00"  # TimesNet æ¯”è¾ƒæ…¢ï¼Œè®¾é•¿ä¸€ç‚¹
CORES = 8  # TimesNet éœ€è¦æ›´å¤š CPU æ ¸å¿ƒ

# æ˜¯å¦å¼ºåˆ¶é‡è·‘
FORCE_RUN = False


# ================= ğŸ› ï¸ æ ¸å¿ƒé€»è¾‘ =================


def get_dataset_list(filepath):
    """è¯»å–æ•°æ®é›†åˆ—è¡¨æ–‡ä»¶"""
    if not os.path.exists(filepath):
        print(f"[Error] Dataset list file not found: {filepath}")
        sys.exit(1)

    datasets = []
    with open(filepath) as f:
        for line in f:
            line = line.strip()
            if line and not line.startswith("#"):
                datasets.append(line)
    return datasets


def get_timesnet_args(dataset_name):
    """
    æ›¿ä»£ Bash è„šæœ¬ä¸­çš„ grep/cut é€»è¾‘ã€‚
    è¯»å– JSON é…ç½®æ–‡ä»¶å¹¶è¿”å›å‚æ•°å­—ç¬¦ä¸²ã€‚
    """
    config_file = os.path.join(CONFIG_DIR, f"{dataset_name}_config.json")

    if not os.path.exists(config_file):
        print(f"[Warning] Config file not found: {config_file}")
        return None

    try:
        with open(config_file) as f:
            cfg = json.load(f)

        # æå–å‚æ•°ï¼Œæ„å»º argument string
        # å¯¹åº” bash: --e_layers ${e_layers} ...
        args = (
            f"--e_layers {cfg.get('e_layers')} "
            f"--batch_size {cfg.get('batch_size')} "
            f"--d_model {cfg.get('d_model')} "
            f"--d_ff {cfg.get('d_ff')} "
            f"--top_k {cfg.get('top_k')} "
            f"--des '{cfg.get('des')}' "
            f"--itr {cfg.get('itr')} "
            f"--learning_rate {cfg.get('learning_rate')} "
            f"--train_epochs {cfg.get('train_epochs')} "
            f"--patience {cfg.get('patience')}"
        )
        return args

    except Exception as e:
        print(f"[Error] Failed to parse json {config_file}: {e}")
        return None


def submit_jobs():
    # 1. è¯»å–æ•°æ®é›†
    datasets = get_dataset_list(DATASET_LIST_FILE)
    print(f"Loaded {len(datasets)} datasets")
    print(f"Classifier: {CLASSIFIER}")
    print(f"Seed/Resample ID: {SEED_SUFFIX}")
    print("=" * 60)

    total_submitted = 0
    total_skipped = 0

    for dataset_name in datasets:
        # 1. è·å– TimesNet çš„é¢å¤–å‚æ•°
        extra_args = get_timesnet_args(dataset_name)
        if extra_args is None:
            print(f"[Skip] Skipping {dataset_name} due to missing config.")
            continue

        # 2. æ„å»º Checkpoint ç›®å½•
        ckpt_dir = os.path.join(CKPT_ROOT, f"{dataset_name}{SEED_SUFFIX}")

        if not os.path.exists(ckpt_dir):
            print(f"[Skip] No checkpoint dir for {dataset_name}")
            continue

        try:
            files = [f for f in os.listdir(ckpt_dir) if f.endswith(".ckpt")]
            files.sort()
        except Exception as e:
            continue

        if not files:
            continue

        print(f"\nProcessing Dataset: {dataset_name} ({len(files)} checkpoints)")

        # åˆ›å»ºæ—¥å¿—ç›®å½•
        log_dir = f"logs/{dataset_name}/{CLASSIFIER}"
        os.makedirs(log_dir, exist_ok=True)

        for i, ckpt_file in enumerate(files):
            ckpt_full_path = os.path.join(ckpt_dir, ckpt_file)
            ckpt_name_clean = ckpt_file.replace(".ckpt", "").replace("=", "_")

            # å˜ä½“åç§° (ä¹Ÿæ˜¯ç»“æœæ–‡ä»¶å¤¹åç§°)
            variant_name = f"lgd_prior_v{i}"

            # ç»“æœè·¯å¾„
            this_result_dir = os.path.join(BASE_RESULTS_ROOT, variant_name) + "/"

            # === æ£€æŸ¥ç»“æœæ˜¯å¦å­˜åœ¨ ===
            # TimesNet çš„è¾“å‡º CSV
            expected_csv = os.path.join(
                this_result_dir,
                CLASSIFIER,
                "Predictions",
                dataset_name,
                f"testResample{SEED_SUFFIX}.csv",
            )

            if os.path.exists(expected_csv) and not FORCE_RUN:
                total_skipped += 1
                continue

            # Job Name
            job_name = f"{dataset_name[:10]}_{CLASSIFIER}_{ckpt_name_clean[-5:]}"

            # SLURM è„šæœ¬å†…å®¹
            job_content = f"""#!/bin/bash
#SBATCH --partition={QUEUE_GPU}
#SBATCH --job-name={job_name}
#SBATCH --output={log_dir}/{ckpt_name_clean}.out
#SBATCH --error={log_dir}/{ckpt_name_clean}.err
#SBATCH --time={TIME}
#SBATCH --mem={MEM}
#SBATCH --cpus-per-task={CORES}
#SBATCH --nodes=1
#SBATCH --gres=gpu:1
#SBATCH --account=ecs

module purge
module load conda
source activate {ENV_DL}

export PYTHONPATH=/home/{USERNAME}/tsml-eval
# === ä¼ é€’ Checkpoint è·¯å¾„ ===
export LGD_VAE_CHECKPOINT_PATH="{ckpt_full_path}"

# è¿è¡Œ TimesNet Python è„šæœ¬
# æ³¨æ„è¿™é‡Œè¿½åŠ äº† extra_args
python -u {SCRIPT_PATH} \
    {DATA_DIR_BASE} \
    {this_result_dir} \
    {CLASSIFIER} \
    {dataset_name} \
    {SEED_SUFFIX} \
    -dtn lgd_prior \
    -tto \
    {extra_args}
"""
            # å†™å…¥å¹¶æäº¤
            sub_file = f"temp_{job_name}.sh"
            with open(sub_file, "w") as f:
                f.write(job_content)

            subprocess.run(["sbatch", sub_file])
            os.remove(sub_file)
            total_submitted += 1

    print("=" * 60)
    print(
        f"Summary: Submitted {total_submitted} jobs, Skipped {total_skipped} existing jobs."
    )


if __name__ == "__main__":
    submit_jobs()
